"""
Enhanced Data Tool - Phase 3

Advanced data tool integrating:
- Multi-source data aggregation
- Real-time data collection
- Enhanced knowledge base
- Data validation and caching
"""

from crewai.tools import BaseTool
from pydantic import BaseModel
from typing import Type, Dict, Any, Optional
import asyncio
import json
import logging
from datetime import datetime

try:
    from ..data_integration import (
        RealtimeDataCollector,
        DataValidator,
        CacheManager,
        MultiSourceAggregator
    )
    DATA_INTEGRATION_AVAILABLE = True
except ImportError:
    DATA_INTEGRATION_AVAILABLE = False
    print("Warning: Data integration modules not available")

class EnhancedDataInput(BaseModel):
    """Input schema for enhanced data tool."""
    symbol: str

class EnhancedDataTool(BaseTool):
    """Enhanced data collection tool with multi-source aggregation."""
    
    name: str = "Enhanced Multi-Source Financial Data Tool"
    description: str = (
        "Advanced tool for collecting comprehensive financial data from multiple sources "
        "with real-time updates, data validation, and quality assurance. "
        "Provides aggregated, validated data with confidence scores."
    )
    args_schema: Type[BaseModel] = EnhancedDataInput
    
    def __init__(self):
        """Initialize enhanced data tool."""
        super().__init__()
        self.logger = logging.getLogger(__name__)
        self.cache_manager = None
        self.aggregator = None
        self.validator = None
        self._initialized = False
        
        # Load enhanced knowledge base
        self.industry_benchmarks = self._load_industry_benchmarks()
    
    async def _initialize_components(self):
        """Initialize data integration components."""
        if not DATA_INTEGRATION_AVAILABLE or self._initialized:
            return
        
        try:
            # Initialize cache manager
            self.cache_manager = CacheManager(
                max_memory_size=50 * 1024 * 1024,  # 50MB
                default_ttl=300  # 5 minutes
            )
            
            # Initialize aggregator
            self.aggregator = MultiSourceAggregator(self.cache_manager)
            await self.aggregator.initialize()
            
            # Initialize validator
            self.validator = DataValidator()
            
            self._initialized = True
            self.logger.info("Enhanced data tool components initialized")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize components: {e}")
    
    def _run(self, symbol: str) -> str:
        """
        Collect enhanced financial data for a symbol.
        
        Args:
            symbol: Stock symbol
            
        Returns:
            Comprehensive financial data analysis
        """
        try:
            # Run async data collection
            return asyncio.run(self._run_async(symbol))
            
        except Exception as e:
            self.logger.error(f"Error in enhanced data collection: {e}")
            return f"L·ªói khi thu th·∫≠p d·ªØ li·ªáu n√¢ng cao cho {symbol}: {str(e)}"
    
    async def _run_async(self, symbol: str) -> str:
        """Async implementation of data collection."""
        try:
            await self._initialize_components()
            
            # Collect data from multiple sources
            if self.aggregator:
                aggregated_data = await self.aggregator.aggregate_stock_data(
                    symbol, 
                    data_types=['price', 'ratios', 'sentiment']
                )
                
                if aggregated_data:
                    return self._format_enhanced_output(symbol, aggregated_data)
            
            # Fallback to basic data collection
            return self._get_basic_data(symbol)
            
        except Exception as e:
            self.logger.error(f"Error in async data collection: {e}")
            return self._get_basic_data(symbol)
    
    def _format_enhanced_output(self, symbol: str, data) -> str:
        """Format enhanced data output."""
        try:
            output = f"""=== PH√ÇN T√çCH D·ªÆ LI·ªÜU N√ÇNG CAO - {symbol.upper()} ===

üìä TH√îNG TIN C∆† B·∫¢N:
‚Ä¢ M√£ c·ªï phi·∫øu: {symbol.upper()}
‚Ä¢ Th·ªùi gian c·∫≠p nh·∫≠t: {data.timestamp.strftime('%Y-%m-%d %H:%M:%S')}
‚Ä¢ ƒêi·ªÉm ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu: {data.data_quality_score:.1%}
‚Ä¢ S·ªë ngu·ªìn d·ªØ li·ªáu: {len(data.source_data)}

üí∞ D·ªÆ LI·ªÜU GI√Å (MULTI-SOURCE):
"""
            
            # Price data
            if 'price' in data.primary_data:
                price = data.primary_data['price']
                change = data.primary_data.get('change', 0)
                change_pct = data.primary_data.get('change_percent', 0)
                volume = data.primary_data.get('volume', 0)
                
                output += f"""‚Ä¢ Gi√° hi·ªán t·∫°i: {price:,.0f} VND
‚Ä¢ Thay ƒë·ªïi: {change:+,.0f} VND ({change_pct:+.2f}%)
‚Ä¢ Kh·ªëi l∆∞·ª£ng: {volume:,} c·ªï phi·∫øu
"""
            
            # Data sources used
            output += f"\nüîó NGU·ªíN D·ªÆ LI·ªÜU S·ª¨ D·ª§NG:\n"
            for source_name, source_data in data.source_data.items():
                confidence = data.confidence_scores.get(source_name, 0)
                output += f"‚Ä¢ {source_name.upper()}: ƒê·ªô tin c·∫≠y {confidence:.1%}\n"
            
            # Conflicts and quality
            if data.conflicts_detected:
                output += f"\n‚ö†Ô∏è XUNG ƒê·ªòT D·ªÆ LI·ªÜU PH√ÅT HI·ªÜN:\n"
                for conflict in data.conflicts_detected:
                    output += f"‚Ä¢ {conflict}\n"
            
            # Sentiment analysis
            if 'sentiment_score' in data.primary_data:
                sentiment = data.primary_data['sentiment_score']
                sentiment_trend = data.primary_data.get('sentiment_trend', 'neutral')
                
                output += f"""
üí≠ PH√ÇN T√çCH T√ÇM L√ù TH·ªä TR∆Ø·ªúNG:
‚Ä¢ ƒêi·ªÉm sentiment: {sentiment:.2f}
‚Ä¢ Xu h∆∞·ªõng: {sentiment_trend.upper()}
"""
            
            # Industry benchmarks
            industry_data = self._get_industry_context(symbol)
            if industry_data:
                output += f"\nüè≠ TH√îNG TIN NG√ÄNH:\n{industry_data}"
            
            # Data quality assessment
            output += f"""
‚úÖ ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU:
‚Ä¢ T·ªïng ƒëi·ªÉm ch·∫•t l∆∞·ª£ng: {data.data_quality_score:.1%}
‚Ä¢ Ph∆∞∆°ng ph√°p x·ª≠ l√Ω xung ƒë·ªôt: {data.resolution_method.value if data.resolution_method else 'N/A'}
‚Ä¢ ƒê·ªô tin c·∫≠y t·ªïng th·ªÉ: {'CAO' if data.data_quality_score > 0.8 else 'TRUNG B√åNH' if data.data_quality_score > 0.6 else 'TH·∫§P'}

üìà KHUY·∫æN NGH·ªä S·ª¨ D·ª§NG:
"""
            
            if data.data_quality_score > 0.8:
                output += "‚Ä¢ D·ªØ li·ªáu c√≥ ƒë·ªô tin c·∫≠y cao, ph√π h·ª£p cho ph√¢n t√≠ch v√† ra quy·∫øt ƒë·ªãnh\n"
            elif data.data_quality_score > 0.6:
                output += "‚Ä¢ D·ªØ li·ªáu c√≥ ƒë·ªô tin c·∫≠y trung b√¨nh, n√™n k·∫øt h·ª£p v·ªõi c√°c ngu·ªìn kh√°c\n"
            else:
                output += "‚Ä¢ D·ªØ li·ªáu c√≥ ƒë·ªô tin c·∫≠y th·∫•p, c·∫ßn th·∫≠n tr·ªçng khi s·ª≠ d·ª•ng\n"
            
            if data.conflicts_detected:
                output += "‚Ä¢ Ph√°t hi·ªán xung ƒë·ªôt d·ªØ li·ªáu, ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω t·ª± ƒë·ªông\n"
            
            output += f"\n‚è±Ô∏è D·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p v√† x·ª≠ l√Ω l√∫c: {datetime.now().strftime('%H:%M:%S')}"
            
            return output
            
        except Exception as e:
            self.logger.error(f"Error formatting enhanced output: {e}")
            return f"L·ªói ƒë·ªãnh d·∫°ng d·ªØ li·ªáu n√¢ng cao cho {symbol}: {str(e)}"
    
    def _get_basic_data(self, symbol: str) -> str:
        """Fallback basic data collection."""
        try:
            # This would use the original vnstock integration
            output = f"""=== D·ªÆ LI·ªÜU C∆† B·∫¢N - {symbol.upper()} ===

‚ö†Ô∏è Ch·∫ø ƒë·ªô d·ªØ li·ªáu c∆° b·∫£n (Enhanced features kh√¥ng kh·∫£ d·ª•ng)

‚Ä¢ M√£ c·ªï phi·∫øu: {symbol.upper()}
‚Ä¢ Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

üí° ƒê·ªÉ s·ª≠ d·ª•ng ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng n√¢ng cao, vui l√≤ng c√†i ƒë·∫∑t:
- Multi-source data integration
- Real-time data collectors
- Data validation system
"""
            
            # Add industry context if available
            industry_data = self._get_industry_context(symbol)
            if industry_data:
                output += f"\nüè≠ TH√îNG TIN NG√ÄNH:\n{industry_data}"
            
            return output
            
        except Exception as e:
            return f"L·ªói thu th·∫≠p d·ªØ li·ªáu c∆° b·∫£n cho {symbol}: {str(e)}"
    
    def _load_industry_benchmarks(self) -> Dict[str, Any]:
        """Load enhanced industry benchmarks."""
        try:
            import os
            benchmark_path = os.path.join(
                os.path.dirname(__file__), 
                '..', '..', '..', 
                'knowledge', 
                'enhanced_industry_benchmarks.json'
            )
            
            if os.path.exists(benchmark_path):
                with open(benchmark_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            return {}
            
        except Exception as e:
            self.logger.warning(f"Could not load industry benchmarks: {e}")
            return {}
    
    def _get_industry_context(self, symbol: str) -> str:
        """Get industry context for symbol."""
        try:
            if not self.industry_benchmarks:
                return ""
            
            # Map common symbols to industries (simplified)
            symbol_industry_map = {
                'HPG': 'Kim lo·∫°i v√† khai kho√°ng',
                'VIC': 'B·∫•t ƒë·ªông s·∫£n',
                'VCB': 'T√†i ch√≠nh ng√¢n h√†ng',
                'FPT': 'Ph·∫ßn m·ªÅm v√† d·ªãch v·ª• c√¥ng ngh·ªá th√¥ng tin',
                'MSN': 'B·∫•t ƒë·ªông s·∫£n',
                'VHM': 'B·∫•t ƒë·ªông s·∫£n',
                'TCB': 'T√†i ch√≠nh ng√¢n h√†ng',
                'SSI': 'Ch·ª©ng kho√°n v√† ng√¢n h√†ng ƒë·∫ßu t∆∞'
            }
            
            industry = symbol_industry_map.get(symbol.upper())
            if not industry:
                return "‚Ä¢ Th√¥ng tin ng√†nh: Ch∆∞a x√°c ƒë·ªãnh"
            
            industries_data = self.industry_benchmarks.get('industries', {})
            industry_info = industries_data.get(industry, {})
            
            if not industry_info:
                return f"‚Ä¢ Ng√†nh: {industry} (Ch∆∞a c√≥ d·ªØ li·ªáu benchmark)"
            
            context = f"‚Ä¢ Ng√†nh: {industry}\n"
            context += f"‚Ä¢ P/E trung b√¨nh ng√†nh: {industry_info.get('PE', 'N/A')}\n"
            context += f"‚Ä¢ P/B trung b√¨nh ng√†nh: {industry_info.get('PB', 'N/A')}\n"
            context += f"‚Ä¢ ROE trung b√¨nh ng√†nh: {industry_info.get('ROE', 'N/A')}%\n"
            
            volatility = industry_info.get('volatility', 'unknown')
            context += f"‚Ä¢ ƒê·ªô bi·∫øn ƒë·ªông: {volatility.upper()}\n"
            
            regulatory_impact = industry_info.get('regulatory_impact', 'unknown')
            context += f"‚Ä¢ T√°c ƒë·ªông ch√≠nh s√°ch: {regulatory_impact.upper()}\n"
            
            return context
            
        except Exception as e:
            self.logger.error(f"Error getting industry context: {e}")
            return "‚Ä¢ L·ªói khi l·∫•y th√¥ng tin ng√†nh"

# Example usage
def test_enhanced_data_tool():
    """Test the enhanced data tool."""
    tool = EnhancedDataTool()
    result = tool._run("HPG")
    print(result)

if __name__ == "__main__":
    test_enhanced_data_tool()
